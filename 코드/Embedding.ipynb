{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean word embedding\n",
    "\n",
    "한국어 데이터셋을 읽고 word score 계산, tokenizing, word2vec 모델을 학습시키고 단어에 대한 벡터를 반환하는 클래스를 구현합니다.\n",
    "\n",
    "## 준비\n",
    "아래의 코드를 돌리기 위해서는 3가지의 pip install이 필요합니다\n",
    "\n",
    "    pip install soynlp\n",
    "    pip install gensim\n",
    "    pip install numpy==1.13, should downgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle\n",
    "\n",
    "# pip install soynlp\n",
    "# pip install gensim\n",
    "# pip install numpy==1.13, should downgrade numpy\n",
    "\n",
    "# https://github.com/lovit/soynlp/\n",
    "# https://lovit.github.io/nlp/2018/04/09/three_tokenizers_soynlp/\n",
    "# https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# https://rutumulkar.com/blog/2015/word2vec\n",
    "\n",
    "# 띄어쓰기 오류의 해결 (맞춤법 교정 툴 사용)\n",
    "# 학습되지 않은 단어에 대한 vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "Embedding 내부의 tokenizer, word2vec 모델을 훈련시키기 위해서는 setWord2Vec 함수의 호출이 필요합니다.\n",
    "이때, 함수의 매개변수로 데이터셋의 파일 이름이 필요하며, 기본적으로 .xlsx 파일을 사용하도록 되어있습니다.\n",
    "\n",
    "함수 실행 시, 읽어온 데이터셋을 바탕으로 word score 계산 -> tokenizer 훈련 -> word2vec 모델 훈련이 이루어지게 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def setWord2Vec(self, fileName):\n",
    "        question = pd.read_excel(fileName + '.xlsx')['question']\n",
    "        print(' read question data from ', fileName)        \n",
    "        for i in range(0,len(question)):\n",
    "            question[i] = self.onlyKorean(question[i])\n",
    "            \n",
    "        word_scores = self.calWordScores(question)\n",
    "        self.tokenizer = self.trainTokenizer(word_scores)        \n",
    "        self.word2vec = self.trainWord2Vec(question)\n",
    "        \n",
    "    def calWordScores(self, question):   \n",
    "        word_extractor = WordExtractor(\n",
    "            max_left_length=20, \n",
    "            max_right_length=20, \n",
    "            min_frequency = 20,\n",
    "            min_cohesion_forward = 0.05,\n",
    "            min_right_branching_entropy = 0.0\n",
    "        )\n",
    "        \n",
    "        word_extractor.train(question)   \n",
    "        word_scores = word_extractor.extract()\n",
    "        print(' extract and calculate ', len(word_scores), ' words')\n",
    "        return word_scores\n",
    "    \n",
    "    def onlyKorean(self, sentence):\n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "    \n",
    "    def trainTokenizer(self, word_scores):\n",
    "        cohesion_scores = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "        tokenizer = MaxScoreTokenizer(scores = cohesion_scores)\n",
    "        # tokenizer = LTokenizer(scores = cohesion_scores)\n",
    "        print(' train tokenizer')  \n",
    "        return tokenizer\n",
    "                \n",
    "    def trainWord2Vec(self, question):\n",
    "        # print(self.question)\n",
    "        tQuestion = [self.tokenizeSentence(q) for q in question]\n",
    "        \n",
    "        word2vec = Word2Vec(\n",
    "            tQuestion, \n",
    "            size = 50, \n",
    "            window = 2, \n",
    "            min_count = 1, \n",
    "            iter = 100, \n",
    "            sg = 1\n",
    "        )\n",
    "        print(' train word2vec') \n",
    "        return word2vec\n",
    "    \n",
    "    # sent는 하나의 문장\n",
    "    def tokenizeSentence(self, sent): \n",
    "        return self.tokenizer.tokenize(sent)\n",
    "    \n",
    "    # words 는 단어들의 리스트 [\"김동호\", \"교수님\"]\n",
    "    def vectorizeWord(self, words):          \n",
    "        return self.word2vec.wv[words] \n",
    "    \n",
    "    def vectorizeSentence(self, sent):\n",
    "        result = []\n",
    "        for s in sent:\n",
    "            s = self.onlyKorean(s)\n",
    "            tSent = self.tokenizeSentence(s) \n",
    "            vec = self.vectorizeWord(tSent)\n",
    "            result.append(torch.FloatTensor(vec))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 객체 생성\n",
    "\n",
    "현재, '통합_181204.xlsx' 데이터셋을 사용하도록 되어있습니다. 다른 데이터셋을 사용하고자 하실 경우 아래의 셀에서 파일명을 수정하여 주시기 바랍니다. 데이터셋과 본 코드는 같은 디렉토리에 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " read question data from  통합_181204\n",
      "training was done. used memory 0.149 Gbry 0.149 Gb\n",
      "all cohesion probabilities was computed. # words = 193\n",
      "all branching entropies was computed # words = 2017\n",
      "all accessor variety was computed # words = 2017\n",
      " extract and calculate  232  words\n",
      " train tokenizer\n",
      " train word2vec\n"
     ]
    }
   ],
   "source": [
    "embed = Embedding()\n",
    "\n",
    "embed.setWord2Vec(\"통합_181204\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(embed, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
