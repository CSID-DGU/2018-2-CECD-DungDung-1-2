{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean word embedding\n",
    "\n",
    "한국어 데이터셋을 읽고 word score 계산, tokenizing, word2vec 모델을 학습시키고 단어에 대한 벡터를 반환하는 클래스를 구현합니다.\n",
    "\n",
    "## 준비\n",
    "아래의 코드를 돌리기 위해서는 3가지의 pip install이 필요합니다\n",
    "\n",
    "    pip install soynlp\n",
    "    pip install gensim\n",
    "    pip install numpy==1.13, should downgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle\n",
    "\n",
    "# pip install soynlp\n",
    "# pip install gensim\n",
    "# pip install numpy==1.13, should downgrade numpy\n",
    "\n",
    "# https://github.com/lovit/soynlp/\n",
    "# https://lovit.github.io/nlp/2018/04/09/three_tokenizers_soynlp/\n",
    "# https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# https://rutumulkar.com/blog/2015/word2vec\n",
    "\n",
    "# 띄어쓰기 오류의 해결 (맞춤법 교정 툴 사용)\n",
    "# 학습되지 않은 단어에 대한 vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding class 정의 \n",
    "\n",
    "Embedding 내부의 tokenizer, word2vec 모델을 훈련시키기 위해서는 setWord2Vec 함수의 호출이 필요합니다.\n",
    "이때, 함수의 매개변수로 데이터셋의 파일 이름이 필요하며, 기본적으로 .xlsx 파일을 사용하도록 되어있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    # embedding 모델 정의, 학습\n",
    "    def setWord2Vec(self, fileName):\n",
    "        question = pd.read_excel(fileName + '.xlsx')['question']\n",
    "        print(' read question data from ', fileName)        \n",
    "        for i in range(0,len(question)):\n",
    "            question[i] = self.onlyKorean(question[i])\n",
    "            \n",
    "        word_scores = self.calWordScores(question)\n",
    "        self.tokenizer = self.trainTokenizer(word_scores)        \n",
    "        self.word2vec = self.trainWord2Vec(question)\n",
    "        \n",
    "    # embedding 모델 학습을 위해 데이터 셋에 등장하는 단어의 점수 계산\n",
    "    def calWordScores(self, question):   \n",
    "        word_extractor = WordExtractor(\n",
    "            max_left_length=20, \n",
    "            max_right_length=20, \n",
    "            min_frequency = 20,\n",
    "            min_cohesion_forward = 0.05,\n",
    "            min_right_branching_entropy = 0.0\n",
    "        )        \n",
    "        word_extractor.train(question)   \n",
    "        word_scores = word_extractor.extract()\n",
    "        print(' extract and calculate ', len(word_scores), ' words')\n",
    "        return word_scores\n",
    "    \n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):\n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "    \n",
    "    # Tokenizer 정의, 학습\n",
    "    def trainTokenizer(self, word_scores):\n",
    "        cohesion_scores = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "        tokenizer = MaxScoreTokenizer(scores = cohesion_scores)\n",
    "        # tokenizer = LTokenizer(scores = cohesion_scores)\n",
    "        print(' train tokenizer')  \n",
    "        return tokenizer\n",
    "                \n",
    "    # word2vec 모델 정의, 학습\n",
    "    def trainWord2Vec(self, question):\n",
    "        # print(self.question)\n",
    "        tQuestion = [self.tokenizeSentence(q) for q in question]\n",
    "        \n",
    "        word2vec = Word2Vec(\n",
    "            tQuestion, \n",
    "            size = 50, \n",
    "            window = 2, \n",
    "            min_count = 1, \n",
    "            iter = 100, \n",
    "            sg = 1\n",
    "        )\n",
    "        print(' train word2vec') \n",
    "        return word2vec\n",
    "    \n",
    "    # 매개변수로 하나의 문장을 받아 tokenize, 결과 반환\n",
    "    def tokenizeSentence(self, sent): \n",
    "        return self.tokenizer.tokenize(sent)\n",
    "    \n",
    "    # 매개변수로 단어들의 리스트(ex.[\"김동호\", \"교수님\"])를 받아 vector 반환\n",
    "    def vectorizeWord(self, words):\n",
    "        result = []\n",
    "        for w in words:\n",
    "            # word2vec.wv.vocab에 있는 단어일 때\n",
    "            try:\n",
    "                result.append(self.word2vec.wv[w])\n",
    "            # word2vec.wv.vocab에 없는 단어일 때\n",
    "            except:\n",
    "                vocab = self.word2vec.wv.vocab\n",
    "                d = len(w)\n",
    "                new_word = \"\"\n",
    "                for v in vocab:\n",
    "                    distance = jamo_levenshtein(v, w)\n",
    "                    if distance < d:\n",
    "                        d = distance\n",
    "                        new_word = v\n",
    "                # 유사한 단어가 있을 때\n",
    "                if new_word != \"\" and d <= 0.7:\n",
    "                    result.append(self.word2vec.wv[new_word])\n",
    "                # 유사한 단어가 없을 때\n",
    "                else\n",
    "                    return \n",
    "        return result\n",
    "    \n",
    "    # 매개변수로 문장들의 리스트를 받아 텐서의 리스트 형태로 vector 반환\n",
    "    def vectorizeSentence(self, sent):\n",
    "        result = []\n",
    "        for s in sent:\n",
    "            s = self.onlyKorean(s)\n",
    "            tSent = self.tokenizeSentence(s) \n",
    "            vec = self.vectorizeWord(tSent)\n",
    "            result.append(torch.FloatTensor(vec))\n",
    "        return result\n",
    "    \n",
    "    # word2vec 에서 처리 가능한 단어의 수 반환\n",
    "    def vocabSize(self):\n",
    "        return len(self.word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed 객체 생성\n",
    "\n",
    "현재, '통합_181204.xlsx' 데이터셋을 사용하도록 되어있습니다. 다른 데이터셋을 사용하고자 하실 경우 아래의 셀에서 파일명을 수정하여 주시기 바랍니다. 데이터셋과 본 코드는 같은 디렉토리에 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed 객체 내의 모델 학습\n",
    "Embedding 내부의 tokenizer, word2vec 모델을 훈련시키기 위해서는 setWord2Vec 함수의 호출이 필요합니다.\n",
    "이때, 함수의 매개변수로 데이터셋의 파일 이름이 필요하며, 기본적으로 .xlsx 파일을 사용하도록 되어있습니다.\n",
    "\n",
    "함수 실행 시, 읽어온 데이터셋을 바탕으로 word score 계산 -> tokenizer 훈련 -> word2vec 모델 훈련이 이루어지게 됩니다\n",
    "\n",
    "embed.vectoriazeSentence([list of sentence]) 함수 호출 시 각 문장에대한 vector를 list of tensor의 형태로 반환합니다. 이때, word vector의 크기는 50입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " read question data from  통합_181204\n",
      "training was done. used memory 0.163 Gbry 0.162 Gb\n",
      "all cohesion probabilities was computed. # words = 193\n",
      "all branching entropies was computed # words = 2017\n",
      "all accessor variety was computed # words = 2017\n",
      " extract and calculate  232  words\n",
      " train tokenizer\n",
      " train word2vec\n"
     ]
    }
   ],
   "source": [
    "embed.setWord2Vec(\"통합_181204\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed 객체의 pickling\n",
    "embed 객체를 pickle화 시켜 .pkl 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(embed, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
