{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, fileName, corpusFileName = \"./korean_embedding/train_data.corpus\", \n",
    "                 logFileName = \"./korean_embedding/training_log/kor2vec_log\", vocabFileName = \"./korean_embedding/vocab.txt\",\n",
    "                tokenizerFileName = \"./korean_embedding/tokenizer.pkl\", kor2vecFileName = \"./korean_embedding/embedding.model\"):\n",
    "        self.fileName = fileName\n",
    "        self.corpusFileName = corpusFileName\n",
    "        self.logFileName = logFileName\n",
    "        self.vocabFileName = vocabFileName\n",
    "        self.tokenizerFileName = tokenizerFileName\n",
    "        self.kor2vecFileName = kor2vecFileName\n",
    "        \n",
    "    # Tokenizer와 Kor2Vec pickling\n",
    "    def setEverything(self):\n",
    "        self.readDataset()\n",
    "        self.setTokenizer()     \n",
    "        self.makeCorpusFile()        \n",
    "        self.makeVocabFile()\n",
    "        self.setKor2Vec()\n",
    "        \n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):    \n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "    \n",
    "    def readDataset(self):\n",
    "        self.question = pd.read_excel(self.fileName + '.xlsx')['question']\n",
    "        print(' read question data from ', self.fileName)        \n",
    "        for i in range(0,len(self.question)):\n",
    "            self.question[i] = self.onlyKorean(self.question[i])\n",
    "        \n",
    "        print('delete punctuation marks from data')\n",
    "            \n",
    "    # question(list of sentence)에 등장하는 단어의 점수 계산\n",
    "    def calWordScores(self):   \n",
    "        word_extractor = WordExtractor(\n",
    "            max_left_length=20, \n",
    "            max_right_length=20, \n",
    "            min_frequency = 20,\n",
    "            min_cohesion_forward = 0.05,\n",
    "            min_right_branching_entropy = 0.0\n",
    "        )        \n",
    "        word_extractor.train(self.question)   \n",
    "        word_scores = word_extractor.extract()\n",
    "        print('extract and calculate ', len(word_scores), ' words')\n",
    "        return word_scores\n",
    "    \n",
    "    # Tokenizer 정의 및 훈련\n",
    "    def setTokenizer(self):\n",
    "        print(' set Tokenizer')        \n",
    "        word_scores = self.calWordScores()\n",
    "        self.tokenizer = self.trainTokenizer(word_scores)    \n",
    "        with open(self.tokenizerFileName, 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        print('Tokenizer saved in ',self.tokenizerFileName)   \n",
    "            \n",
    "    # Tokenizer 훈련\n",
    "    def trainTokenizer(self, word_scores):\n",
    "        cohesion_scores = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "        tokenizer = MaxScoreTokenizer(scores = cohesion_scores)\n",
    "        # tokenizer = LTokenizer(scores = cohesion_scores)\n",
    "        print('train tokenizer')  \n",
    "        return tokenizer\n",
    "    \n",
    "    def makeCorpusFile(self):\n",
    "        print(' make corpus file')   \n",
    "        sample = []\n",
    "        for q in self.question:\n",
    "            words = self.tokenizer.tokenize(q)\n",
    "            sentence = \" \".join(words)\n",
    "            sample.append(sentence)\n",
    "        f = codecs.open(self.corpusFileName, 'w', encoding='utf8')\n",
    "        for s in sample:\n",
    "            f.write(s + \"\\r\\n\")\n",
    "        f.close() \n",
    "        print('corpus file saved in ', self.corpusFileName) \n",
    "        \n",
    "    def makeVocabFile(self):\n",
    "        print(' make vocab file')   \n",
    "        vocab = []\n",
    "        for q in self.question:\n",
    "            words = self.tokenizer.tokenize(q)\n",
    "            for w in words:\n",
    "                if w not in vocab:\n",
    "                    vocab.append(w)\n",
    "            \n",
    "        f = open(self.vocabFileName, 'w')\n",
    "        for v in vocab:\n",
    "            f.write(v + \"\\n\")\n",
    "        f.close() \n",
    "        print('vocab file saved in ', self.vocabFileName) \n",
    "            \n",
    "    def setKor2Vec(self):\n",
    "        self.kor2vec = Kor2Vec(embed_size=128)\n",
    "        self.kor2vec.train(self.corpusFileName, self.logFileName, batch_size=128)\n",
    "        self.kor2vec.save(self.kor2vecFileName) # saving embedding\n",
    "        print('Kor2Vec saved in ', self.kor2vecFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " read question data from  ./dataset/2019_01_06_10차_RAN\n",
      "delete punctuation marks from data\n",
      " set Tokenizer\n",
      "training was done. used memory 0.145 Gbry 0.139 Gb\n",
      "all cohesion probabilities was computed. # words = 2043\n",
      "all branching entropies was computed # words = 4634\n",
      "all accessor variety was computed # words = 4634\n",
      "extract and calculate  1031  words\n",
      "train tokenizer\n",
      "Tokenizer saved in  ./korean_embedding/tokenizer.pkl\n",
      " make corpus file\n",
      "corpus file saved in  ./korean_embedding/train_data.corpus\n",
      " make vocab file\n",
      "vocab file saved in  ./korean_embedding/vocab.txt\n",
      "Reading Corpus lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spliting Lines: 100%|███████████████████████████████████████████████████████████| 5424/5424 [00:00<00:00, 94314.59it/s]\n",
      "Corpus Sampling: 100%|███████████████████████████████████████████████████████████| 5424/5424 [00:01<00:00, 5128.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kor2vec\n",
      "Loading Word_sample corpus\n",
      "Loading corpus finished\n",
      "CUDA Available/count: False 0\n",
      "training on  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 0: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:25<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_ep_loss': 1.640703519641376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 1: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:27<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'train_ep_loss': 1.1105290965955765}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 2: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:21<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'train_ep_loss': 1.046385027932339}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 3: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:16<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'train_ep_loss': 1.0149116371498734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 4: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:14<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'train_ep_loss': 0.9907120352885762}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 5: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:15<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'train_ep_loss': 0.9692070892599762}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 6: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:14<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'train_ep_loss': 0.9514710465415579}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 7: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:14<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'train_ep_loss': 0.9296866516597936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 8: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:15<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'train_ep_loss': 0.9136798186380355}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 9: 100%|██████████████████████████████████████████████████████████████████████████| 305/305 [01:13<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'train_ep_loss': 0.898882993322904}\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"./dataset/2019_01_06_10차_RAN\")\n",
    "vocab.setEverything()\n",
    "# embedding dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = vocab.kor2vec.embedding(\"김동호 교수님 수업 어때?\")\n",
    "\n",
    "#input = vocab.kor2vec.to_seqs([\"김동호 교수님 수업 어때?\", \"컴퓨터보안\"], seq_len=6)\n",
    "#vocab.kor2vec.forward(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
