{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 10\n",
    "# 서버로부터 질문 문장을 받을때는 텍스트 파일에 있는 문장을 읽어오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "               torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "    \n",
    "    # x = embedding.vectorizeSentence(list of sentence)\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        lstm_out = lstm_out[:,9,:]\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        \n",
    "        # y = self.hidden2label(lstm_out, -1)\n",
    "        result = F.log_softmax(y, dim=1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMaker:\n",
    "    def __init__(self, kor2vecFileName = \"./nlp/embedding.model\", tokenizerFileName = \"./nlp/tokenizer.pkl\", \n",
    "                 calssifierFileName = \"./nlp/classifier.model\", vocabFileName = \"./nlp/vocab.txt\", seq_len = SEQ_LEN):\n",
    "        self.setKor2Vec(kor2vecFileName)\n",
    "        self.setTokenizer(tokenizerFileName)\n",
    "        self.setClassifier(calssifierFileName)\n",
    "        self.setVocab(vocabFileName)\n",
    "        \n",
    "        self.seq_len =seq_len\n",
    "        \n",
    "    def setKor2Vec(self, kor2vecFileName):\n",
    "        self.kor2vec = Kor2Vec.load(kor2vecFileName)\n",
    "        \n",
    "    def setTokenizer(self, tokenizerFileName):\n",
    "        with open(tokenizerFileName,'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "    \n",
    "    def setClassifier(self, calssifierFileName):\n",
    "        with open(calssifierFileName,'rb') as f:\n",
    "            self.classifier = pickle.load(f)\n",
    "        \n",
    "    def setVocab(self, vocabFileName):\n",
    "        self.vocab = []\n",
    "        f = open(vocabFileName, 'r')\n",
    "        \n",
    "        while True:\n",
    "            word = f.readline()\n",
    "            if not word: \n",
    "                break\n",
    "            else :\n",
    "                self.vocab.append(word[:-1])\n",
    "        f.close()        \n",
    "        \n",
    "    def tokenizeSentence(self, sentence): \n",
    "        sentence = sentence.repalce(\" \", \"\")\n",
    "        result = self.tokenizer.tokenize(sentence)\n",
    "        return result\n",
    "    \n",
    "    def checkOOV(self, words):\n",
    "        new_words = []\n",
    "        for w in words:\n",
    "            if w in self.vocab:\n",
    "                new_words.append(w)\n",
    "            else:\n",
    "                baseline = 0.7\n",
    "                new_w = \"\"\n",
    "                for v in self.vocab:\n",
    "                    distance = jamo_levenshtein(v, w)\n",
    "                    if distance <= baseline:\n",
    "                        baseline = distance\n",
    "                        new_w = v\n",
    "                # 유사한 단어가 있을 때\n",
    "                if new_w != \"\" and baseline <= 0.7:\n",
    "                    new_words.append(new_w)\n",
    "                # 유사한 단어가 없을 때\n",
    "                else:\n",
    "                    new_words.append(w)\n",
    "        # print(new_words)\n",
    "        return new_words\n",
    "\n",
    "    def deleteSymbol(self, sentence):\n",
    "        f = re.compile('[^ ㄱ-ㅣ가-힣|A-Z|a-z|0-9 ]+') \n",
    "        result = f.sub('', sentence)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def onlyKorean(self, sentence):\n",
    "        f = re.compile('[^ ㄱ-ㅣ가-힣 ]+') \n",
    "        result = f.sub('', sentence)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def vectorizeSentence(self, sentence):\n",
    "        x = self.kor2vec.to_seqs(sentence, seq_len = self.seq_len)\n",
    "        x = self.kor2vec(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def modelForward(self, vectors):\n",
    "        self.classifier.hidden = (torch.zeros(1, 1, self.classifier.hidden_dim), torch.zeros(1, 1, self.classifier.hidden_dim))\n",
    "        result = self.classifier.forward(vectors)\n",
    "        _, result = torch.max(result, 1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # 원문장에서 기호 삭제한 문장, 원문장에서 기호 및 외국어 삭제한 문장, 레이블\n",
    "    def classifySentence(self, sentence):    \n",
    "        noSymbol_sentence = self.deleteSymbol(sentence)\n",
    "        \n",
    "        korean_sentence = self.onlyKorean(sentence)\n",
    "        words = self.tokenizeSentence(korean_sentence)\n",
    "        words = self.checkOOV(words)\n",
    "        fixed_sentence = \" \".join(words)\n",
    "        vectors = self.vectorizeSentence([new_sentence])\n",
    "        \n",
    "        \"\"\"\n",
    "        original_words = noSymbol.split(\" \") \n",
    "        original_words = self.checkOOV(original_words)\n",
    "        oovChecked = \" \".join(original_words)\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.modelForward(vectors)\n",
    "        \n",
    "        return noSymbol_sentence, fixed_sentence, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerMaker:\n",
    "    def __init__(self, professorFileName, lectureFileName, shortLectureFileName, answerFolderName):\n",
    "        self.professorFileName = professorFileName\n",
    "        self.lectureFileName = lectureFileName\n",
    "        self.shortLectureFileName = shortLectureFileName\n",
    "        \n",
    "        self.answerFolderName = answerFolderName\n",
    "        \n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.where.html\n",
    "    def getAnswer(self, noSymbolSentence, koreanSentense, oovCheckedSentence, label):\n",
    "        \n",
    "    \n",
    "    def readKeywordData(self, label):\n",
    "        # 강의명\n",
    "        if label == \"0\" or label == \"1\" or label == \"4\":\n",
    "            self.keywords = pd.read_excel(self.lectureFileName)[\"name\"]\n",
    "            self.short_keywords = pd.read_excel(self.shortLectureFileName)\n",
    "            \n",
    "        # 교수명\n",
    "        elif label == \"3\" or label == \"5\" or label == \"6\":\n",
    "            self.keywords = pd.read_excel(self.professorFileName)[\"name\"]\n",
    "            \n",
    "        # 강의명 & 교수명\n",
    "        elif label == \"2\":\n",
    "            self.keywords1 = pd.read_excel(self.professorFileName)[\"name\"]\n",
    "            self.keywords2 = pd.read_excel(self.lectureFileName)[\"name\"]\n",
    "            self.short_keywords = pd.read_excel(self.shortLectureFileName)\n",
    "    \n",
    "    def readAnswerData(self, label):        \n",
    "        self.answers = pd.read_excel(self.answerFolderName + \"/\" + label + \".xlsx\") \n",
    "        \n",
    "    def changeAbbreviation(self, sentence):\n",
    "        \n",
    "    \n",
    "    def findKeyword(self, noSymbolSentence, koreanSentense, oovCheckedSentence, label):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능은 어떤 교수님이 하시나요 인공지능은 어떤 교수님이 하시나요 인공지능은 어떤 교수님이 하시나요 tensor([1])\n"
     ]
    }
   ],
   "source": [
    "lm = LabelMaker()\n",
    "noSymbol, korean, oovChecked, result = lm.classifySentence(\"인공지능은 어떤 교수님이 하시나요?\")\n",
    "\n",
    "print(noSymbol, korean, oovChecked, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-071e8c2d65df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAnswerMaker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./answerData/교수명.xlsx\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./answerData/강의명.xlsx\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./answerData/강의명_줄임말.xlsx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./answerData\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAnswer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoSymbol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkorean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moovChecked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-27600043ca9d>\u001b[0m in \u001b[0;36mgetAnswer\u001b[1;34m(self, noSymbolSentence, koreanSentense, oovCheckedSentence, label)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# 줄임말 체크\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"0\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"1\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"4\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mnoSymbolSentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchangeAbbreviation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoSymbolSentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mkoreanSentense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchangeAbbreviation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkoreanSentense\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0moovCheckedSentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchangeAbbreviation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moovCheckedSentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-27600043ca9d>\u001b[0m in \u001b[0;36mchangeAbbreviation\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchangeAbbreviation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshort_keywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshort_keywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"short\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshort_keywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"short\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "am = AnswerMaker(\"./answerData/교수명.xlsx\",\"./answerData/강의명.xlsx\",\"./answerData/강의명_줄임말.xlsx\", \"./answerData\")\n",
    "answer = am.getAnswer(noSymbol, korean, oovChecked, result)\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
