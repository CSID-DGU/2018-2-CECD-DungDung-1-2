{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 준비된 데이터셋을 사용하여 vocab, tokenizer, kor2vec, Classifier를 학습, 저장하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainVocab:\n",
    "    def __init__(self, fileName, corpusFileName = \"./nlp/train_data.corpus\", \n",
    "                 logFileName = \"./nlp/training_log/kor2vec_log\", vocabFileName = \"./nlp/vocab.txt\",\n",
    "                tokenizerFileName = \"./nlp/tokenizer.pkl\", kor2vecFileName = \"./nlp/embedding.model\"):\n",
    "        self.fileName = fileName\n",
    "        self.corpusFileName = corpusFileName\n",
    "        self.logFileName = logFileName\n",
    "        self.vocabFileName = vocabFileName\n",
    "        self.tokenizerFileName = tokenizerFileName\n",
    "        self.kor2vecFileName = kor2vecFileName\n",
    "        \n",
    "    # Tokenizer와 Kor2Vec pickling\n",
    "    def setEverything(self):\n",
    "        self.readDataset()\n",
    "        self.setTokenizer()     \n",
    "        self.makeCorpusFile()        \n",
    "        self.makeVocabFile()\n",
    "        self.setKor2Vec()\n",
    "        \n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):    \n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣 ]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "    \n",
    "    def readDataset(self):\n",
    "        self.question = pd.read_excel(self.fileName)['question']\n",
    "        print(' read question data from ', self.fileName)        \n",
    "        for i in range(0,len(self.question)):\n",
    "            self.question[i] = self.onlyKorean(self.question[i])\n",
    "        \n",
    "        print('delete punctuation marks from data')\n",
    "            \n",
    "    # question(list of sentence)에 등장하는 단어의 점수 계산\n",
    "    def calWordScores(self):   \n",
    "        word_extractor = WordExtractor(\n",
    "            max_left_length=20, \n",
    "            max_right_length=20, \n",
    "            min_frequency = 30,\n",
    "            min_cohesion_forward = 0.05,\n",
    "            min_right_branching_entropy = 0.0\n",
    "        )        \n",
    "        word_extractor.train(self.question)   \n",
    "        word_scores = word_extractor.extract()\n",
    "        print('extract and calculate ', len(word_scores), ' words')\n",
    "        return word_scores\n",
    "    \n",
    "    # Tokenizer 정의 및 훈련\n",
    "    def setTokenizer(self):\n",
    "        print(' set Tokenizer')        \n",
    "        word_scores = self.calWordScores()\n",
    "        self.tokenizer = self.trainTokenizer(word_scores)    \n",
    "        with open(self.tokenizerFileName, 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        print('Tokenizer saved in ',self.tokenizerFileName)   \n",
    "            \n",
    "    # Tokenizer 훈련\n",
    "    def trainTokenizer(self, word_scores):\n",
    "        cohesion_scores = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "        tokenizer = MaxScoreTokenizer(scores = cohesion_scores)\n",
    "        # tokenizer = LTokenizer(scores = cohesion_scores)\n",
    "        print('train tokenizer')  \n",
    "        return tokenizer\n",
    "    \n",
    "    def makeCorpusFile(self):\n",
    "        print(' make corpus file')   \n",
    "        sample = []\n",
    "        for q in self.question:\n",
    "            words = self.tokenizer.tokenize(q)\n",
    "            sentence = \" \".join(words)\n",
    "            sample.append(sentence)\n",
    "        f = codecs.open(self.corpusFileName, 'w', encoding='utf8')\n",
    "        for s in sample:\n",
    "            f.write(s + \"\\r\\n\")\n",
    "        f.close() \n",
    "        print('corpus file saved in ', self.corpusFileName) \n",
    "        \n",
    "    def makeVocabFile(self):\n",
    "        print(' make vocab file')   \n",
    "        vocab = []\n",
    "        for q in self.question:\n",
    "            q = q.replace(\" \", \"\")\n",
    "            words = self.tokenizer.tokenize(q)\n",
    "            for w in words:\n",
    "                if w not in vocab:\n",
    "                    vocab.append(w)\n",
    "            \n",
    "        f = open(self.vocabFileName, 'w')\n",
    "        for v in vocab:\n",
    "            f.write(v + \"\\n\")\n",
    "        f.close() \n",
    "        print('vocab file saved in ', self.vocabFileName) \n",
    "            \n",
    "    def setKor2Vec(self):\n",
    "        self.kor2vec = Kor2Vec(embed_size=128)\n",
    "        self.kor2vec.train(self.corpusFileName, self.logFileName, batch_size=128)\n",
    "        self.kor2vec.save(self.kor2vecFileName) # saving embedding\n",
    "        print('Kor2Vec saved in ', self.kor2vecFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " read question data from  ./dataset/TotalDataset.xlsx\n",
      "delete punctuation marks from data\n",
      " set Tokenizer\n",
      "training was done. used memory 0.163 Gbory 0.149 Gb\n",
      "all cohesion probabilities was computed. # words = 2530\n",
      "all branching entropies was computed # words = 8882\n",
      "all accessor variety was computed # words = 8882\n",
      "extract and calculate  1473  words\n",
      "train tokenizer\n",
      "Tokenizer saved in  ./nlp/tokenizer.pkl\n",
      " make corpus file\n",
      "corpus file saved in  ./nlp/train_data.corpus\n",
      " make vocab file\n",
      "vocab file saved in  ./nlp/vocab.txt\n",
      "Reading Corpus lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spliting Lines: 100%|████████████████████████████████████████████████████████| 15732/15732 [00:00<00:00, 192347.44it/s]\n",
      "Corpus Sampling: 100%|█████████████████████████████████████████████████████████| 15732/15732 [00:02<00:00, 6052.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kor2vec\n",
      "Loading Word_sample corpus\n",
      "Loading corpus finished\n",
      "CUDA Available/count: False 0\n",
      "training on  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 0: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_ep_loss': 1.3620213416626412}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 1: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:17<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'train_ep_loss': 1.084679505106304}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 2: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:16<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'train_ep_loss': 1.0286114386828298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 3: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:32<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'train_ep_loss': 0.994489808481528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 4: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:14<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'train_ep_loss': 0.9701039365642415}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 5: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:23<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'train_ep_loss': 0.9521244436621191}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 6: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:24<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'train_ep_loss': 0.9323679735144454}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 7: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:23<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'train_ep_loss': 0.9172853212432558}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 8: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'train_ep_loss': 0.9033639309494935}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP 9: 100%|██████████████████████████████████████████████████████████████████████████| 753/753 [03:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'train_ep_loss': 0.890222750534891}\n",
      "Kor2Vec saved in  ./nlp/embedding.model\n"
     ]
    }
   ],
   "source": [
    "# datasetFileName = sys.argv[1]\n",
    "vocab = TrainVocab(\"./dataset/TotalDataset.xlsx\")\n",
    "# vocab = TrainVocab(datasetFileName)\n",
    "vocab.setEverything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 64\n",
    "LABEL_SIZE = 7\n",
    "BATCH_SIZE = 23\n",
    "EPOCH = 15\n",
    "\n",
    "'''\n",
    "Epoch = 50\n",
    "    model test result :  2618\n",
    "    2618\n",
    "    model test result :  2618 / 2622\n",
    "    99.84744469870328 %\n",
    "'''\n",
    "\n",
    "SEQ_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(D.Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.fileName = fileName\n",
    "        label = pd.read_excel(self.fileName)['label']\n",
    "        sentence = pd.read_excel(self.fileName)['question']        \n",
    "        print(' set dataset')\n",
    "        print('read data from ', self.fileName)\n",
    "        \n",
    "        for i in range(0,len(sentence)):\n",
    "            sentence[i] = self.onlyKorean(sentence[i])\n",
    "        print('delete punctuation marks from data')\n",
    "        \n",
    "        self.len = len(sentence)\n",
    "        self.x_data = sentence.values   \n",
    "        self.y_data = label.values\n",
    "    \n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):    \n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣 ]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        result.replace(\" \", \"\")\n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 모델은 따로\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "               torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "    \n",
    "    # x = embedding.vectorizeSentence(list of sentence)\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        lstm_out = lstm_out[:,9,:]\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        \n",
    "        # y = self.hidden2label(lstm_out, -1)\n",
    "        result = F.log_softmax(y, dim=1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel():\n",
    "    def __init__(self, fileName, \n",
    "                 vocabFileName = \"./nlp/vocab.txt\", tokenizerFileName = \"./nlp/tokenizer.pkl\", \n",
    "                 kor2vecFileName = \"./nlp/embedding.model\", classifierFileName = \"./nlp/classifier.model\", \n",
    "                 embedding_dim = EMBEDDING_DIM, hidden_size = HIDDEN_SIZE, label_size = LABEL_SIZE, epoch = EPOCH, seq_len = SEQ_LEN):\n",
    "        \n",
    "        self.fileName = fileName\n",
    "        self.vocabFileName = vocabFileName\n",
    "        self.tokenizerFileName = tokenizerFileName\n",
    "        self.kor2vecFileName = kor2vecFileName  \n",
    "        self.classifierFileName = classifierFileName\n",
    "       \n",
    "        self.readNLP()\n",
    "        self.readDataset()        \n",
    "        \n",
    "        self.model = SentenceClassifier(embedding_dim, hidden_size, label_size)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.1)\n",
    "        self.seq_len = seq_len\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def trainStart(self):\n",
    "        self.trainModel()\n",
    "        self.saveModel()\n",
    "        \n",
    "    # tokenizer, kor2vec, vocab 불러오기\n",
    "    def readNLP(self):\n",
    "        # tokenizer 사용하는 이유 = 띄어쓰기 문제 해결을 위하여\n",
    "        with open(self.tokenizerFileName,'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "            \n",
    "        # model\n",
    "        self.kor2vec = Kor2Vec.load(self.kor2vecFileName)\n",
    "        \n",
    "        # vocab\n",
    "        self.vocab = []\n",
    "        f = open(self.vocabFileName, 'r')\n",
    "        while True:\n",
    "            word = f.readline()\n",
    "            if not word: \n",
    "                break\n",
    "            else :\n",
    "                self.vocab.append(word[:-1])\n",
    "        f.close()\n",
    "    \n",
    "    def readDataset(self):    \n",
    "        self.dataset = SentenceDataset(self.fileName)\n",
    "        \n",
    "        # train, test 나누기\n",
    "        train_len = (self.dataset.__len__() / 6) * 5\n",
    "        train_len = int(round(float(train_len)))\n",
    "        test_len = self.dataset.__len__() - train_len\n",
    "        \n",
    "        print(\"train len : \", train_len)\n",
    "        print(\"test len : \", test_len)\n",
    "        \n",
    "        self.train_data, self.test_data = D.random_split(self.dataset, lengths=[train_len, test_len])\n",
    "        \n",
    "        self.train_loader = D.DataLoader(dataset = self.train_data,\n",
    "                                  batch_size = BATCH_SIZE,\n",
    "                                  shuffle = True)\n",
    "        self.test_loader = D.DataLoader(dataset = self.test_data,\n",
    "                                  batch_size = BATCH_SIZE,\n",
    "                                  shuffle = True)\n",
    "    \n",
    "    def trainModel(self):\n",
    "        # training\n",
    "        for e in range(self.epoch):\n",
    "            for i, data in enumerate(self.train_loader, 0):\n",
    "                x = list(data[0])\n",
    "                y = data[1]\n",
    "                x = self.kor2vec.to_seqs(x, seq_len = self.seq_len) \n",
    "                # tensor(batch_size, seq_len, char_seq_len)\n",
    "                x = self.kor2vec(x) \n",
    "                # tensor(batch_size, seq_len, 128)\n",
    "                \n",
    "                self.model.zero_grad()\n",
    "                self.model.hidden = self.model.init_hidden()\n",
    "                # run our forward pass.\n",
    "                result = self.model(x)\n",
    "                \n",
    "                # compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = self.loss_function(result, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            print(e, \"(loss : \", loss, \")\")\n",
    "                \n",
    "    def makeLabeltoTensor(self, label):\n",
    "        result = torch.zeros(0, 0)\n",
    "        \n",
    "        for l in label:\n",
    "            temp = torch.zeros([1,7], dtype=torch.long)\n",
    "            temp[0][int(l)] = 1\n",
    "            if result.size() == torch.Size([0, 0]):\n",
    "                result = temp\n",
    "            else:\n",
    "                result = torch.cat([result, temp], dim=0)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def saveModel(self):\n",
    "        with open(self.classifierFileName, 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "            \n",
    "    def test(self):\n",
    "        correct = 0\n",
    "        all = 0\n",
    "        # test\n",
    "        for i, data in enumerate(self.test_loader, 0):\n",
    "            x = list(data[0])\n",
    "            y = data[1]\n",
    "            x = self.kor2vec.to_seqs(x, seq_len = self.seq_len)\n",
    "            # tensor(batch_size, seq_len, char_seq_len)\n",
    "            x = self.kor2vec(x) \n",
    "            # tensor(batch_size, seq_len, 128)\n",
    "\n",
    "            self.model.hidden = self.model.init_hidden()\n",
    "            result = self.model(x)\n",
    "\n",
    "            _, result = torch.max(result, 1)\n",
    "\n",
    "            for i in range(len(data[0])):\n",
    "                all += 1\n",
    "                if result[i] == y[i]:\n",
    "                    correct += 1\n",
    "\n",
    "        print(\"model test result : \", correct)\n",
    "        print(correct)\n",
    "        print(\"model test result : \", correct, \"/\", all)\n",
    "        print((correct / all) * 100, \"%\")\n",
    "        \n",
    "    def debug(self):\n",
    "        print(\"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " set dataset\n",
      "read data from  ./dataset/TotalDataset.xlsx\n",
      "delete punctuation marks from data\n",
      "train len :  13110\n",
      "test len :  2622\n",
      "0 (loss :  tensor(1.0175, grad_fn=<NllLossBackward>) )\n",
      "1 (loss :  tensor(0.1352, grad_fn=<NllLossBackward>) )\n",
      "2 (loss :  tensor(0.2829, grad_fn=<NllLossBackward>) )\n",
      "3 (loss :  tensor(0.0957, grad_fn=<NllLossBackward>) )\n",
      "4 (loss :  tensor(0.0043, grad_fn=<NllLossBackward>) )\n",
      "5 (loss :  tensor(0.0048, grad_fn=<NllLossBackward>) )\n",
      "6 (loss :  tensor(0.0048, grad_fn=<NllLossBackward>) )\n",
      "7 (loss :  tensor(0.0018, grad_fn=<NllLossBackward>) )\n",
      "8 (loss :  tensor(0.0131, grad_fn=<NllLossBackward>) )\n",
      "9 (loss :  tensor(0.0010, grad_fn=<NllLossBackward>) )\n",
      "10 (loss :  tensor(0.0007, grad_fn=<NllLossBackward>) )\n",
      "11 (loss :  tensor(0.0012, grad_fn=<NllLossBackward>) )\n",
      "12 (loss :  tensor(0.0004, grad_fn=<NllLossBackward>) )\n",
      "13 (loss :  tensor(0.0004, grad_fn=<NllLossBackward>) )\n",
      "14 (loss :  tensor(0.0005, grad_fn=<NllLossBackward>) )\n"
     ]
    }
   ],
   "source": [
    "# tm = TrainModel(fileName = \"./dataset/2019_01_06_10차_RAN.xlsx\")\n",
    "tm = TrainModel(fileName = \"./dataset/TotalDataset.xlsx\")\n",
    "tm.trainStart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model test result :  2616\n",
      "2616\n",
      "model test result :  2616 / 2622\n",
      "99.77116704805492 %\n"
     ]
    }
   ],
   "source": [
    "tm.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
