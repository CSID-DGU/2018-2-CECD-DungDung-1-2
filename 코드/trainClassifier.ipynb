{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 64\n",
    "LABEL_SIZE = 7\n",
    "BATCH_SIZE = 54\n",
    "EPOCH = 50\n",
    "SEQ_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(D.Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.fileName = fileName\n",
    "        label = pd.read_excel(self.fileName + '.xlsx')['label']\n",
    "        sentence = pd.read_excel(self.fileName + '.xlsx')['question']        \n",
    "        print(' set dataset')\n",
    "        print('read data from ', self.fileName)\n",
    "        \n",
    "        for i in range(0,len(sentence)):\n",
    "            sentence[i] = self.onlyKorean(sentence[i])\n",
    "        print('delete punctuation marks from data')\n",
    "        \n",
    "        self.len = len(sentence)\n",
    "        self.x_data = sentence.values   \n",
    "        self.y_data = label.values\n",
    "    \n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):    \n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 모델은 따로\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "               torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "    \n",
    "    # x = embedding.vectorizeSentence(list of sentence)\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        lstm_out = lstm_out[:,9,:]\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        \n",
    "        # y = self.hidden2label(lstm_out, -1)\n",
    "        result = F.log_softmax(y, dim=1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel():\n",
    "    def __init__(self, fileName, \n",
    "                 vocabFileName = \"./nlp/vocab.txt\", tokenizerFileName = \"./nlp/tokenizer.pkl\", \n",
    "                 kor2vecFileName = \"./nlp/embedding.model\", classifierFileName = \"./nlp/classifier.model\", \n",
    "                 embedding_dim = EMBEDDING_DIM, hidden_size = HIDDEN_SIZE, label_size = LABEL_SIZE, epoch = EPOCH, seq_len = SEQ_LEN):\n",
    "        \n",
    "        self.fileName = fileName\n",
    "        self.vocabFileName = vocabFileName\n",
    "        self.tokenizerFileName = tokenizerFileName\n",
    "        self.kor2vecFileName = kor2vecFileName  \n",
    "        self.classifierFileName = classifierFileName\n",
    "       \n",
    "        self.readNLP()\n",
    "        self.readDataset()        \n",
    "        \n",
    "        self.model = SentenceClassifier(embedding_dim, hidden_size, label_size)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.1)\n",
    "        self.seq_len = seq_len\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def trainStart(self):\n",
    "        self.trainModel()\n",
    "        self.saveModel()\n",
    "        \n",
    "    # tokenizer, kor2vec, vocab 불러오기\n",
    "    def readNLP(self):\n",
    "        # tokenizer 사용하는 이유 = 띄어쓰기 문제 해결을 위하여\n",
    "        with open(self.tokenizerFileName,'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "            \n",
    "        # model\n",
    "        self.kor2vec = Kor2Vec.load(self.kor2vecFileName)\n",
    "        \n",
    "        # vocab\n",
    "        self.vocab = []\n",
    "        f = open(self.vocabFileName, 'r')\n",
    "        while True:\n",
    "            word = f.readline()\n",
    "            if not word: \n",
    "                break\n",
    "            else :\n",
    "                self.vocab.append(word[:-1])\n",
    "        f.close()\n",
    "    \n",
    "    def readDataset(self):    \n",
    "        self.dataset = SentenceDataset(self.fileName)\n",
    "        \n",
    "        # train, test 나누기\n",
    "        train_len = self.dataset.__len__() * 0.8\n",
    "        train_len = int(round(float(train_len)))\n",
    "        test_len = self.dataset.__len__() - train_len\n",
    "        \n",
    "        print(\"train len : \", train_len)\n",
    "        print(\"test len : \", test_len)\n",
    "        \n",
    "        self.train_data, self.test_data = D.random_split(self.dataset, lengths=[train_len, test_len])\n",
    "        \n",
    "        self.train_loader = D.DataLoader(dataset = self.train_data,\n",
    "                                  batch_size = BATCH_SIZE,\n",
    "                                  shuffle = True)\n",
    "        self.test_loader = D.DataLoader(dataset = self.test_data,\n",
    "                                  batch_size = BATCH_SIZE,\n",
    "                                  shuffle = True)\n",
    "    \n",
    "    def trainModel(self):\n",
    "        # training\n",
    "        for e in range(self.epoch):\n",
    "            for i, data in enumerate(self.train_loader, 0):\n",
    "                x = list(data[0])\n",
    "                y = data[1]\n",
    "                x = self.kor2vec.to_seqs(x, seq_len = self.seq_len) # tensor(batch_size, seq_len, char_seq_len)\n",
    "                x = self.kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "                # x = self.kor2vec.embedding(x)\n",
    "                \n",
    "                # y = self.makeLabeltoTensor(y)\n",
    "                \n",
    "                # clear gradients out before each instance\n",
    "                self.model.zero_grad()\n",
    "                self.model.hidden = self.model.init_hidden()\n",
    "                # run our forward pass.\n",
    "                result = self.model(x)\n",
    "                \n",
    "                # compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = self.loss_function(result, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            print(e, \"(loss : \", loss, \")\")\n",
    "                \n",
    "    def makeLabeltoTensor(self, label):\n",
    "        result = torch.zeros(0, 0)\n",
    "        \n",
    "        for l in label:\n",
    "            temp = torch.zeros([1,7], dtype=torch.long)\n",
    "            temp[0][int(l)] = 1\n",
    "            if result.size() == torch.Size([0, 0]):\n",
    "                result = temp\n",
    "            else:\n",
    "                result = torch.cat([result, temp], dim=0)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def saveModel(self):\n",
    "        with open(self.classifierFileName, 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "            \n",
    "    def test(self):\n",
    "        correct = 0\n",
    "        all = 0\n",
    "        # test\n",
    "        for i, data in enumerate(self.test_loader, 0):\n",
    "            x = list(data[0])\n",
    "            y = data[1]\n",
    "            x = self.kor2vec.to_seqs(x, seq_len = self.seq_len) # tensor(batch_size, seq_len, char_seq_len)\n",
    "            x = self.kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "\n",
    "            self.model.hidden = self.model.init_hidden()\n",
    "            result = self.model(x)\n",
    "\n",
    "            _, result = torch.max(result, 1)\n",
    "\n",
    "            for i in range(len(data[0])):\n",
    "                all += 1\n",
    "                if result[i] == y[i]:\n",
    "                    correct += 1\n",
    "\n",
    "        print(\"model test result : \", correct)\n",
    "        print(correct)\n",
    "        print(\"model test result : \", correct, \"/\", all)\n",
    "        print((correct / all) * 100, \"%\")\n",
    "        \n",
    "    def debug(self):\n",
    "        print(\"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kor2vec = Kor2Vec.load(\"../model/path\")\n",
    "## or kor2vec = SejongVector()\n",
    "\n",
    "#lstm = nn.LSTM(128, 64, batch_first=True)\n",
    "#dense = nn.Linear(64, 1)\n",
    "\n",
    "## Make tensor input\n",
    "#sentences = [\"이 영화는 정말 대박이에요\", \"우와 진짜 재미있었어요\"]\n",
    "\n",
    "#x = kor2vec.to_seqs(sentences, seq_len=10)\n",
    "## >>> tensor(batch_size, seq_len, char_seq_len)\n",
    "\n",
    "#x = kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "#_, (x, xc) = lstm(x) # tensor(batch_size, 64)\n",
    "#x = dense(x) # tensor(batch_size, 1)\n",
    "\n",
    "## test = vocab.kor2vec.embedding(\"김동호 교수님 수업 어때?\")\n",
    "\n",
    "##input = vocab.kor2vec.to_seqs([\"김동호 교수님 수업 어때?\", \"컴퓨터보안\"], seq_len=6)\n",
    "## vocab.kor2vec.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " set dataset\n",
      "read data from  ./dataset/2019_01_06_10차_RAN\n",
      "delete punctuation marks from data\n",
      "train len :  4320\n",
      "test len :  1080\n"
     ]
    }
   ],
   "source": [
    "tm = TrainModel(fileName = \"./dataset/2019_01_06_10차_RAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (loss :  tensor(1.7813, grad_fn=<NllLossBackward>) )\n",
      "1 (loss :  tensor(1.7190, grad_fn=<NllLossBackward>) )\n",
      "2 (loss :  tensor(1.5357, grad_fn=<NllLossBackward>) )\n",
      "3 (loss :  tensor(1.0777, grad_fn=<NllLossBackward>) )\n",
      "4 (loss :  tensor(0.6396, grad_fn=<NllLossBackward>) )\n",
      "5 (loss :  tensor(0.3683, grad_fn=<NllLossBackward>) )\n",
      "6 (loss :  tensor(0.1014, grad_fn=<NllLossBackward>) )\n",
      "7 (loss :  tensor(0.1436, grad_fn=<NllLossBackward>) )\n",
      "8 (loss :  tensor(0.0552, grad_fn=<NllLossBackward>) )\n",
      "9 (loss :  tensor(0.0248, grad_fn=<NllLossBackward>) )\n",
      "10 (loss :  tensor(0.0328, grad_fn=<NllLossBackward>) )\n",
      "11 (loss :  tensor(0.0135, grad_fn=<NllLossBackward>) )\n",
      "12 (loss :  tensor(0.0167, grad_fn=<NllLossBackward>) )\n",
      "13 (loss :  tensor(0.0105, grad_fn=<NllLossBackward>) )\n",
      "14 (loss :  tensor(0.0087, grad_fn=<NllLossBackward>) )\n",
      "15 (loss :  tensor(0.0100, grad_fn=<NllLossBackward>) )\n",
      "16 (loss :  tensor(0.0060, grad_fn=<NllLossBackward>) )\n",
      "17 (loss :  tensor(0.0045, grad_fn=<NllLossBackward>) )\n",
      "18 (loss :  tensor(0.0065, grad_fn=<NllLossBackward>) )\n",
      "19 (loss :  tensor(0.0041, grad_fn=<NllLossBackward>) )\n",
      "20 (loss :  tensor(0.0036, grad_fn=<NllLossBackward>) )\n",
      "21 (loss :  tensor(0.0038, grad_fn=<NllLossBackward>) )\n",
      "22 (loss :  tensor(0.0035, grad_fn=<NllLossBackward>) )\n",
      "23 (loss :  tensor(0.0024, grad_fn=<NllLossBackward>) )\n",
      "24 (loss :  tensor(0.0026, grad_fn=<NllLossBackward>) )\n",
      "25 (loss :  tensor(0.0026, grad_fn=<NllLossBackward>) )\n",
      "26 (loss :  tensor(0.0021, grad_fn=<NllLossBackward>) )\n",
      "27 (loss :  tensor(0.0026, grad_fn=<NllLossBackward>) )\n",
      "28 (loss :  tensor(0.0032, grad_fn=<NllLossBackward>) )\n",
      "29 (loss :  tensor(0.0016, grad_fn=<NllLossBackward>) )\n",
      "30 (loss :  tensor(0.0018, grad_fn=<NllLossBackward>) )\n",
      "31 (loss :  tensor(0.0018, grad_fn=<NllLossBackward>) )\n",
      "32 (loss :  tensor(0.0014, grad_fn=<NllLossBackward>) )\n",
      "33 (loss :  tensor(0.0015, grad_fn=<NllLossBackward>) )\n",
      "34 (loss :  tensor(0.0018, grad_fn=<NllLossBackward>) )\n",
      "35 (loss :  tensor(0.0019, grad_fn=<NllLossBackward>) )\n",
      "36 (loss :  tensor(0.0015, grad_fn=<NllLossBackward>) )\n",
      "37 (loss :  tensor(0.0017, grad_fn=<NllLossBackward>) )\n",
      "38 (loss :  tensor(0.0019, grad_fn=<NllLossBackward>) )\n",
      "39 (loss :  tensor(0.0012, grad_fn=<NllLossBackward>) )\n",
      "40 (loss :  tensor(0.0010, grad_fn=<NllLossBackward>) )\n",
      "41 (loss :  tensor(0.0011, grad_fn=<NllLossBackward>) )\n",
      "42 (loss :  tensor(0.0013, grad_fn=<NllLossBackward>) )\n",
      "43 (loss :  tensor(0.0013, grad_fn=<NllLossBackward>) )\n",
      "44 (loss :  tensor(0.0016, grad_fn=<NllLossBackward>) )\n",
      "45 (loss :  tensor(0.0010, grad_fn=<NllLossBackward>) )\n",
      "46 (loss :  tensor(0.0010, grad_fn=<NllLossBackward>) )\n",
      "47 (loss :  tensor(0.0012, grad_fn=<NllLossBackward>) )\n",
      "48 (loss :  tensor(0.0011, grad_fn=<NllLossBackward>) )\n",
      "49 (loss :  tensor(0.0010, grad_fn=<NllLossBackward>) )\n"
     ]
    }
   ],
   "source": [
    "tm.trainStart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-14353a192a70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-a047bdace80c>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model test result : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "tm.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model test result :  1074\n",
      "1074\n",
      "model test result :  1074 / 1080\n",
      "99.44444444444444 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "all = 0\n",
    "# test\n",
    "for i, data in enumerate(tm.test_loader, 0):\n",
    "    x = list(data[0])\n",
    "    y = data[1]\n",
    "    x = tm.kor2vec.to_seqs(x, seq_len = tm.seq_len) # tensor(batch_size, seq_len, char_seq_len)\n",
    "    x = tm.kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "    \n",
    "    tm.model.hidden = tm.model.init_hidden()\n",
    "    result = tm.model(x)\n",
    "                \n",
    "    _, result = torch.max(result, 1)\n",
    "                \n",
    "    for i in range(len(data[0])):\n",
    "        all += 1\n",
    "        if result[i] == y[i]:\n",
    "            correct += 1\n",
    "        \n",
    "print(\"model test result : \", correct)\n",
    "print(correct)\n",
    "print(\"model test result : \", correct, \"/\", all)\n",
    "print((correct / all) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 0])\n"
     ]
    }
   ],
   "source": [
    "x = [\"김동호 교수님 강의 들을만 해요?\", \"자료구조와실습은 언제 듣는게 좋은가요?\"]\n",
    "\n",
    "x = tm.kor2vec.to_seqs(x, seq_len = tm.seq_len) # tensor(batch_size, seq_len, char_seq_len)\n",
    "x = tm.kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "tm.model.hidden = (torch.zeros(1, 2, tm.model.hidden_dim), torch.zeros(1, 2, tm.model.hidden_dim))\n",
    "result = tm.model.forward(x)\n",
    "                \n",
    "_, result = torch.max(result, 1)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
