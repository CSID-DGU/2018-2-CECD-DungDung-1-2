{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 10\n",
    "LABEL_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(D.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        label = pd.read_excel(self.fileName + '.xlsx')['label']\n",
    "        sentence = pd.read_excel(self.fileName + '.xlsx')['question']        \n",
    "        print(' set dataset')\n",
    "        print('read data from ', fileName)\n",
    "        \n",
    "        for i in range(0,len(sentence)):\n",
    "            sentence[i] = self.onlyKorean(sentence[i])\n",
    "        print('delete punctuation marks from data')\n",
    "        \n",
    "        self.len = len(sentence)\n",
    "        self.x_data = torch.tensor(sentence.values)        \n",
    "        self.y_data = torch.tensor(label.values)   \n",
    "    \n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):    \n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 모델은 따로\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "               torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    # x = embedding.vectorizeSentence(list of sentence)\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel():\n",
    "    def __init__(self, fileName, \n",
    "                 vocabFileName = \"./korean_embedding/vocab.txt\", tokenizerFileName = \"./korean_embedding/tokenizer.pkl\", \n",
    "                 kor2vecFileName = \"./korean_embedding/embedding.model\", \n",
    "                 embedding_dim = EMBEDDING_DIM, hidden_size = HIDDEN_SIZE, label_size = LABEL_SIZE):\n",
    "        \n",
    "        self.fileName = fileName\n",
    "        self.vocabFileName = vocabFileName\n",
    "        self.tokenizerFileName = tokenizerFileName\n",
    "        self.kor2vecFileName = kor2vecFileName       \n",
    "       \n",
    "        self.readNLP()\n",
    "        self.readDataset()\n",
    "        \n",
    "        self.loss_function = nn.NLLLoss()\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "        \n",
    "        self.model = SentenceClassifier(embedding_dim, hidden_size, label_size)\n",
    "        self.trainModel()\n",
    "\n",
    "    # tokenizer, kor2vec, vocab 불러오기\n",
    "    def readNLP(self):\n",
    "        # tokenizer 사용하는 이유 = 띄어쓰기 문제 해결을 위하여\n",
    "        with open('./korean_embedding/tokenizer.pkl','rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "            \n",
    "        # model\n",
    "        self.kor2vec = Kor2Vec.load(\"./korean_embedding/embedding.model\")\n",
    "        \n",
    "        # vocab\n",
    "        self.vocab = []\n",
    "        f = open(\"./korean_embedding/vocab.txt\", 'r')\n",
    "        while True:\n",
    "            word = f.readline()\n",
    "            if not word: \n",
    "                break\n",
    "            else :\n",
    "                self.vocab.append(word[:-1])\n",
    "        f.close()\n",
    "    \n",
    "    def readDataset(self):    \n",
    "        self.dataset = SentenceDataset(fileName)\n",
    "        \n",
    "        # train, test 나누기\n",
    "        train_len = self.dataset.__len__() * 0.8\n",
    "        test_len = self.dataset.__len__() - train_len\n",
    "        \n",
    "        self.train_data, self.test_data = D.random_split(self.dataset, lengths=[train_len, test_len])\n",
    "        \n",
    "        self.train_loader = DataLoader(dataset = self.train_data,\n",
    "                                  batch_size = 32,\n",
    "                                  shuffle = True,\n",
    "                                  num_workers = 2)\n",
    "        self.test_loader = DataLoader(dataset = self.test_data,\n",
    "                                  batch_size = 32,\n",
    "                                  shuffle = True,\n",
    "                                  num_workers = 2)\n",
    "    \n",
    "    def trainModel(self):\n",
    "        # training\n",
    "        for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                x = kor2vec.to_seqs(sentences, seq_len=10) # tensor(batch_size, seq_len, char_seq_len)\n",
    "                x = kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "                \n",
    "                # clear gradients out before each instance\n",
    "                model.zero_grad()\n",
    "                # clear out the hidden state of the LSTM\n",
    "                model.hidden = model.init_hidden()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = model(sentence_in)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kor2vec = Kor2Vec.load(\"../model/path\")\n",
    "## or kor2vec = SejongVector()\n",
    "\n",
    "#lstm = nn.LSTM(128, 64, batch_first=True)\n",
    "#dense = nn.Linear(64, 1)\n",
    "\n",
    "## Make tensor input\n",
    "#sentences = [\"이 영화는 정말 대박이에요\", \"우와 진짜 재미있었어요\"]\n",
    "\n",
    "#x = kor2vec.to_seqs(sentences, seq_len=10)\n",
    "## >>> tensor(batch_size, seq_len, char_seq_len)\n",
    "\n",
    "#x = kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "#_, (x, xc) = lstm(x) # tensor(batch_size, 64)\n",
    "#x = dense(x) # tensor(batch_size, 1)\n",
    "\n",
    "## test = vocab.kor2vec.embedding(\"김동호 교수님 수업 어때?\")\n",
    "\n",
    "##input = vocab.kor2vec.to_seqs([\"김동호 교수님 수업 어때?\", \"컴퓨터보안\"], seq_len=6)\n",
    "## vocab.kor2vec.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_rebuild_parameter' on <module 'torch._utils' from 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\lib\\\\site-packages\\\\torch\\\\_utils.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dcf28e189858>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./dataset/2019_01_06_10차_RAN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-b17a463bb356>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fileName, vocabFileName, tokenizerFileName, kor2vecFileName, embedding_dim, hidden_size, label_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkor2vecFileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkor2vecFileName\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-b17a463bb356>\u001b[0m in \u001b[0;36mreadNLP\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkor2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKor2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./korean_embedding/embedding.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# vocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\kor2vec\\kor2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute '_rebuild_parameter' on <module 'torch._utils' from 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_64\\\\lib\\\\site-packages\\\\torch\\\\_utils.py'>"
     ]
    }
   ],
   "source": [
    "tm = TrainModel(\"./dataset/2019_01_06_10차_RAN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
