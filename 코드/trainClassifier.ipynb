{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 10\n",
    "LABEL_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(D.Dataset):\n",
    "    def __init__(self, fileName):\n",
    "        self.fileName = fileName\n",
    "        label = pd.read_excel(self.fileName + '.xlsx')['label']\n",
    "        sentence = pd.read_excel(self.fileName + '.xlsx')['question']        \n",
    "        print(' set dataset')\n",
    "        print('read data from ', self.fileName)\n",
    "        \n",
    "        for i in range(0,len(sentence)):\n",
    "            sentence[i] = self.onlyKorean(sentence[i])\n",
    "        print('delete punctuation marks from data')\n",
    "        \n",
    "        self.len = len(sentence)\n",
    "        self.x_data = sentence.values   \n",
    "        self.y_data = label.values\n",
    "    \n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):    \n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 모델은 따로\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 10, self.hidden_dim),\n",
    "               torch.zeros(1, 10, self.hidden_dim))\n",
    "    \n",
    "    # x = embedding.vectorizeSentence(list of sentence)\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        # y = self.hidden2label(lstm_out, -1)\n",
    "        result = F.log_softmax(y, dim=1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel():\n",
    "    def __init__(self, fileName, \n",
    "                 vocabFileName = \"./korean_embedding/vocab.txt\", tokenizerFileName = \"./korean_embedding/tokenizer.pkl\", \n",
    "                 kor2vecFileName = \"./korean_embedding/embedding.model\", \n",
    "                 embedding_dim = EMBEDDING_DIM, hidden_size = HIDDEN_SIZE, label_size = LABEL_SIZE):\n",
    "        \n",
    "        self.fileName = fileName\n",
    "        self.vocabFileName = vocabFileName\n",
    "        self.tokenizerFileName = tokenizerFileName\n",
    "        self.kor2vecFileName = kor2vecFileName       \n",
    "       \n",
    "        self.readNLP()\n",
    "        self.readDataset()        \n",
    "        \n",
    "        self.model = SentenceClassifier(embedding_dim, hidden_size, label_size)\n",
    "        \n",
    "        self.loss_function = nn.NLLLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.1)\n",
    "        self.seq_len = 10\n",
    "        \n",
    "        #self.debug()\n",
    "        self.trainModel()\n",
    "\n",
    "    # tokenizer, kor2vec, vocab 불러오기\n",
    "    def readNLP(self):\n",
    "        # tokenizer 사용하는 이유 = 띄어쓰기 문제 해결을 위하여\n",
    "        with open('./korean_embedding/tokenizer.pkl','rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "            \n",
    "        # model\n",
    "        self.kor2vec = Kor2Vec.load(\"./korean_embedding/embedding.model\")\n",
    "        \n",
    "        # vocab\n",
    "        self.vocab = []\n",
    "        f = open(\"./korean_embedding/vocab.txt\", 'r')\n",
    "        while True:\n",
    "            word = f.readline()\n",
    "            if not word: \n",
    "                break\n",
    "            else :\n",
    "                self.vocab.append(word[:-1])\n",
    "        f.close()\n",
    "    \n",
    "    def readDataset(self):    \n",
    "        self.dataset = SentenceDataset(self.fileName)\n",
    "        \n",
    "        # train, test 나누기\n",
    "        train_len = self.dataset.__len__() * 0.8\n",
    "        train_len = int(round(float(train_len)))\n",
    "        test_len = self.dataset.__len__() - train_len\n",
    "        \n",
    "        self.train_data, self.test_data = D.random_split(self.dataset, lengths=[train_len, test_len])\n",
    "        \n",
    "        self.train_loader = D.DataLoader(dataset = self.train_data,\n",
    "                                  batch_size = 32,\n",
    "                                  shuffle = True,\n",
    "                                  num_workers = 0)\n",
    "        self.test_loader = D.DataLoader(dataset = self.test_data,\n",
    "                                  batch_size = 32,\n",
    "                                  shuffle = True,\n",
    "                                  num_workers = 0)\n",
    "    \n",
    "    def trainModel(self):\n",
    "        # training\n",
    "        for epoch in range(100):\n",
    "            for i, data in enumerate(self.train_loader, 0):\n",
    "                x = list(data[0])\n",
    "                y = data[1]\n",
    "                x = self.kor2vec.to_seqs(x, seq_len = self.seq_len) # tensor(batch_size, seq_len, char_seq_len)\n",
    "                x = self.kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "                # x = self.kor2vec.embedding(x)\n",
    "                \n",
    "                y = self.makeLabeltoList(y)\n",
    "                \n",
    "                # clear gradients out before each instance\n",
    "                self.model.zero_grad()\n",
    "                # clear out the hidden state of the LSTM\n",
    "                self.model.hidden = self.model.init_hidden()\n",
    "\n",
    "                # run our forward pass.\n",
    "                result = self.model(x)\n",
    "\n",
    "                # compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = self.loss_function(result, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    def makeLabeltoList(self, label):\n",
    "        result = []\n",
    "        for l in label:\n",
    "            temp = [0,0,0,0,0,0,0]\n",
    "            temp[int(l)] = 1\n",
    "            result.append(temp)\n",
    "            \n",
    "        return result\n",
    "                \n",
    "    def debug(self):\n",
    "        for i, data in enumerate(self.train_loader, 0):\n",
    "            x = list(data[0])\n",
    "            y = data[1]\n",
    "            print(x)\n",
    "            print(y)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kor2vec = Kor2Vec.load(\"../model/path\")\n",
    "## or kor2vec = SejongVector()\n",
    "\n",
    "#lstm = nn.LSTM(128, 64, batch_first=True)\n",
    "#dense = nn.Linear(64, 1)\n",
    "\n",
    "## Make tensor input\n",
    "#sentences = [\"이 영화는 정말 대박이에요\", \"우와 진짜 재미있었어요\"]\n",
    "\n",
    "#x = kor2vec.to_seqs(sentences, seq_len=10)\n",
    "## >>> tensor(batch_size, seq_len, char_seq_len)\n",
    "\n",
    "#x = kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "#_, (x, xc) = lstm(x) # tensor(batch_size, 64)\n",
    "#x = dense(x) # tensor(batch_size, 1)\n",
    "\n",
    "## test = vocab.kor2vec.embedding(\"김동호 교수님 수업 어때?\")\n",
    "\n",
    "##input = vocab.kor2vec.to_seqs([\"김동호 교수님 수업 어때?\", \"컴퓨터보안\"], seq_len=6)\n",
    "## vocab.kor2vec.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " set dataset\n",
      "read data from  ./dataset/2019_01_06_10차_RAN\n",
      "delete punctuation marks from data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dcf28e189858>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./dataset/2019_01_06_10차_RAN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-d467b0a94f40>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fileName, vocabFileName, tokenizerFileName, kor2vecFileName, embedding_dim, hidden_size, label_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#self.debug()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# tokenizer, kor2vec, vocab 불러오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-d467b0a94f40>\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;31m# compute the loss, gradients, and update the parameters by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;31m#  calling optimizer.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1784\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1786\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1787\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m   1788\u001b[0m                          .format(input.size(0), target.size(0)))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "tm = TrainModel(\"./dataset/2019_01_06_10차_RAN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
