{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 10\n",
    "LABEL_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-7-8b6fde7d4865>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-8b6fde7d4865>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def __init__():\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class TrainModel():\n",
    "    def __init__(self, fileName, \n",
    "                 vocabFileName = \"./korean_embedding/vocab.txt\", tokenizerFileName = \"./korean_embedding/tokenizer.pkl\", \n",
    "                 kor2vecFileName = \"./korean_embedding/embedding.model\"):\n",
    "        \n",
    "        self.fileName = fileName\n",
    "        self.vocabFileName = vocabFileName\n",
    "        self.tokenizerFileName = tokenizerFileName\n",
    "        self.kor2vecFileName = kor2vecFileName       \n",
    "       \n",
    "        self.readNLP()\n",
    "        self.readDataset()\n",
    "        \n",
    "        self.loss_function = nn.NLLLoss()\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "        \n",
    "        self.model = SentenceClassifier()\n",
    "        self.trainModel()\n",
    "    # 매개변수로 받은 sentence에서 문장부호를 제외한 한글만 남김\n",
    "    def onlyKorean(self, sentence):    \n",
    "        korean = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "        result = korean.sub('', sentence)\n",
    "        return result\n",
    "\n",
    "    def readDataset(self):\n",
    "        self.sentence = pd.read_excel(self.fileName + '.xlsx')['question']\n",
    "        print(' read question data from ', self.fileName)        \n",
    "        for i in range(0,len(sentence)):\n",
    "            self.sentence[i] = self.onlyKorean(sentence[i])\n",
    "\n",
    "        print('delete punctuation marks from data')\n",
    "        \n",
    "    def readNLP(self):\n",
    "        # tokenizer 사용하는 이유 = 띄어쓰기 문제 해결을 위하여\n",
    "        with open('./korean_embedding/tokenizer.pkl','rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        # model\n",
    "        self.kor2vec = Kor2Vec.load(\"./korean_embedding/embedding.model\")\n",
    "        # vocab\n",
    "        self.vocab = []\n",
    "        f = open(\"./korean_embedding/vocab.txt\", 'r')\n",
    "        while True:\n",
    "            word = f.readline()\n",
    "            if not word: \n",
    "                break\n",
    "            else :\n",
    "                self.vocab.append(word[:-1])\n",
    "        f.close()\n",
    "        \n",
    "    def trainModel(self):\n",
    "        # training\n",
    "        for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "            for sentence, tags in training_data:\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Also, we need to clear out the hidden state of the LSTM,\n",
    "                # detaching it from its history on the last instance.\n",
    "                model.hidden = model.init_hidden()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "                targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = model(sentence_in)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 모델은 따로\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "               torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    # x = embedding.vectorizeSentence(list of sentence)\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor2vec = Kor2Vec.load(\"../model/path\")\n",
    "# or kor2vec = SejongVector()\n",
    "\n",
    "lstm = nn.LSTM(128, 64, batch_first=True)\n",
    "dense = nn.Linear(64, 1)\n",
    "\n",
    "# Make tensor input\n",
    "sentences = [\"이 영화는 정말 대박이에요\", \"우와 진짜 재미있었어요\"]\n",
    "\n",
    "x = kor2vec.to_seqs(sentences, seq_len=10)\n",
    "# >>> tensor(batch_size, seq_len, char_seq_len)\n",
    "\n",
    "x = kor2vec(x) # tensor(batch_size, seq_len, 128)\n",
    "_, (x, xc) = lstm(x) # tensor(batch_size, 64)\n",
    "x = dense(x) # tensor(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = vocab.kor2vec.embedding(\"김동호 교수님 수업 어때?\")\n",
    "\n",
    "# input = vocab.kor2vec.to_seqs([\"김동호 교수님 수업 어때?\", \"컴퓨터보안\"], seq_len=6)\n",
    "# vocab.kor2vec.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset(train/test) 구성\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "model = SentenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
