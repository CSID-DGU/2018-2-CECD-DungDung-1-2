{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor2vec import Kor2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from soynlp.hangle import jamo_levenshtein\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 10\n",
    "# 받을때는 텍스트 파일에 있는 문장을 읽어오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "               torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "    \n",
    "    # x = embedding.vectorizeSentence(list of sentence)\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        lstm_out = lstm_out[:,9,:]\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        \n",
    "        # y = self.hidden2label(lstm_out, -1)\n",
    "        result = F.log_softmax(y, dim=1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, kor2vecFileName = \"./nlp/embedding.model\", tokenizerFileName = \"./nlp/tokenizer.pkl\", \n",
    "                 calssifierFileName = \"./nlp/classifier.model\", vocabFileName = \"./nlp/vocab.txt\", seq_len = SEQ_LEN):\n",
    "        self.setKor2Vec(kor2vecFileName)\n",
    "        self.setTokenizer(tokenizerFileName)\n",
    "        self.setClassifier(calssifierFileName)\n",
    "        self.setVocab(vocabFileName)\n",
    "        \n",
    "        self.seq_len =seq_len\n",
    "        \n",
    "    def setKor2Vec(self, kor2vecFileName):\n",
    "        self.kor2vec = Kor2Vec.load(kor2vecFileName)\n",
    "        \n",
    "    def setTokenizer(self, tokenizerFileName):\n",
    "        with open(tokenizerFileName,'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "    \n",
    "    def setClassifier(self, calssifierFileName):\n",
    "        with open(calssifierFileName,'rb') as f:\n",
    "            self.classifier = pickle.load(f)\n",
    "        \n",
    "    def setVocab(self, vocabFileName):\n",
    "        self.vocab = []\n",
    "        f = open(vocabFileName, 'r')\n",
    "        \n",
    "        while True:\n",
    "            word = f.readline()\n",
    "            if not word: \n",
    "                break\n",
    "            else :\n",
    "                self.vocab.append(word[:-1])\n",
    "        f.close()        \n",
    "        \n",
    "    def tokenizeSentence(self, sentence): \n",
    "        result = self.tokenizer.tokenize(sentence)\n",
    "        return result\n",
    "    \n",
    "    def checkOOV(self, words):\n",
    "        new_words = []\n",
    "        for w in words:\n",
    "            if w in self.vocab:\n",
    "                new_words.append(w)\n",
    "            else:\n",
    "                baseline = 0.7\n",
    "                new_w = \"\"\n",
    "                for v in self.vocab:\n",
    "                    distance = jamo_levenshtein(v, w)\n",
    "                    if distance <= baseline:\n",
    "                        baseline = distance\n",
    "                        new_w = v\n",
    "                # 유사한 단어가 있을 때\n",
    "                if new_w != \"\" and baseline <= 0.7:\n",
    "                    new_words.append(new_w)\n",
    "                # 유사한 단어가 없을 때\n",
    "                else:\n",
    "                    new_words.append(w)\n",
    "        return new_words\n",
    "\n",
    "    def deleteSymbol(self, sentence):\n",
    "        f = re.compile('[^ ㄱ-ㅣ가-힣|A-Z|a-z|0-9 ]+') \n",
    "        result = f.sub('', sentence)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def onlyKorean(self, sentence):\n",
    "        f = re.compile('[^ ㄱ-ㅣ가-힣 ]+') \n",
    "        result = f.sub('', sentence)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def vectorizeSentence(self, sentence):\n",
    "        x = self.kor2vec.to_seqs(sentence, seq_len = self.seq_len)\n",
    "        x = self.kor2vec(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def modelForward(self, vectors):\n",
    "        self.classifier.hidden = (torch.zeros(1, 1, self.classifier.hidden_dim), torch.zeros(1, 1, self.classifier.hidden_dim))\n",
    "        result = self.classifier.forward(vectors)\n",
    "        _, result = torch.max(result, 1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def classifySentence(self, sentence):        \n",
    "        noSymbol = self.deleteSymbol(sentence)\n",
    "        korean = self.onlyKorean(sentence)\n",
    "        words = self.tokenizeSentence(korean)\n",
    "        words = self.checkOOV(words)\n",
    "        new_sentence = \" \".join(words)\n",
    "        vectors = self.vectorizeSentence([new_sentence])\n",
    "        result = self.modelForward(vectors)\n",
    "        \n",
    "        return noSymbol, korean, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('공개SW는 언제 들어야해요', '공개는 언제 들어야해요', tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "print(c.classifySentence(\"공개SW는 언제 들어야해요?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
