{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean word embedding\n",
    "\n",
    "한국어 데이터셋을 읽고 word score 계산, tokenizing, word2vec 모델을 학습시키고 단어에 대한 벡터를 반환하는 클래스를 구현합니다.\n",
    "\n",
    "## 준비\n",
    "아래의 코드를 돌리기 위해서는 3가지의 pip install이 필요합니다\n",
    "\n",
    "    pip install soynlp\n",
    "    pip install gensim\n",
    "    pip install numpy==1.13, should downgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# pip install soynlp\n",
    "# pip install gensim\n",
    "# pip install numpy==1.13, should downgrade numpy\n",
    "\n",
    "# https://github.com/lovit/soynlp/\n",
    "# https://lovit.github.io/nlp/2018/04/09/three_tokenizers_soynlp/\n",
    "# https://ratsgo.github.io/natural%20language%20processing/2017/03/08/word2vec/\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "# 띄어쓰기 오류의 해결 (맞춤법 교정 툴 사용)\n",
    "# 학습되지 않은 단어에 대한 encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedQAData\n",
    "\n",
    "EmbedQAData의 생성자에는 열고자 하는 데이터셋의 파일 이름이 매개변수로 필요하며, 기본적으로 .xlsx 파일을 사용하도록 되어있습니다.\n",
    "\n",
    "생성자 실행 시, 읽어온 데이터셋을 바탕으로 word score 계산 -> tokenizer 설정 -> word2vec 모델 학습 및 저장이 이루어지게 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbedQAData:\n",
    "    def __init__(self, fileName):\n",
    "        self.open_fileName = fileName\n",
    "        self.save_fileName = self.open_fileName + '_word2vec'\n",
    "        \n",
    "        self.question = pd.read_excel(self.open_fileName + '.xlsx')['question']\n",
    "        print(' read question data from ', self.open_fileName)\n",
    "        \n",
    "        self.setWordScores()\n",
    "        self.setTokenizer()\n",
    "        self.saveWord2Vec()\n",
    "        \n",
    "    def setWordScores(self):   \n",
    "        word_extractor = WordExtractor(\n",
    "            max_left_length=20, \n",
    "            max_right_length=20, \n",
    "            min_frequency = 20,\n",
    "            min_cohesion_forward = 0.05,\n",
    "            min_right_branching_entropy = 0.0\n",
    "        )\n",
    "        \n",
    "        word_extractor.train(self.question)\n",
    "        self.word_scores = word_extractor.extract()\n",
    "        print(' extract and calculate ', len(self.word_scores), ' words in ', self.open_fileName)\n",
    "    \n",
    "    def setTokenizer(self):\n",
    "        cohesion_scores = {word:score.cohesion_forward for word, score in self.word_scores.items()}\n",
    "        self.tokenizer = LTokenizer(scores = cohesion_scores)\n",
    "        print(' set tokenizer')\n",
    "        \n",
    "    def tokenizeSentence(self, sent):\n",
    "        tSent = [self.tokenizer.tokenize(s) for s in sent]\n",
    "        '''\n",
    "        result = []\n",
    "        for s in sent:\n",
    "            temp = self.tokenizer.tokenize(s)\n",
    "            result.append(temp)\n",
    "        return result\n",
    "        '''\n",
    "        \n",
    "        return tSent\n",
    "                \n",
    "    def saveWord2Vec(self):\n",
    "        self.tQuestion = self.tokenizeSentence(self.question)\n",
    "        self.word2vec = Word2Vec(\n",
    "            self.tQuestion, \n",
    "            size = 100, \n",
    "            window = 2, \n",
    "            min_count = 30, \n",
    "            iter = 100, \n",
    "            sg = 1\n",
    "        )\n",
    "        \n",
    "        self.word2vec.save(self.save_fileName + '.model')\n",
    "        print(' train word2vec model and save model in ', self.save_fileName)\n",
    "        \n",
    "    def vectorizeWord(self, words):          \n",
    "        return self.word2vec.wv[words]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedQAData 객체 생성\n",
    "\n",
    "현재, '2018_11_10.xlsx' 데이터셋을 사용하도록 되어있습니다. 다른 데이터셋을 사용하고자 하실 경우 아래의 셀에서 파일명을 수정하여 주시기 바랍니다. 데이터셋과 본 코드는 같은 디렉토리에 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " read question data from  2018_11_10\n",
      "training was done. used memory 0.139 Gbry 0.135 Gb\n",
      "all cohesion probabilities was computed. # words = 1592\n",
      "all branching entropies was computed # words = 4190\n",
      "all accessor variety was computed # words = 4190\n",
      " extract and calculate  611  words in  2018_11_10\n",
      " set tokenizer\n",
      " train word2vec model and save model in  2018_11_10_word2vec\n"
     ]
    }
   ],
   "source": [
    "embed = EmbedQAData('2018_11_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedQAData 내의 word2vec 모델 활용\n",
    "\n",
    "embed.word2vec.wv.most_similar(word, topn = n) 함수는 데이터셋 내에 포함된 단어 중, word와 가장 유사한 n개의 단어와 유사도를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('엄기현', 0.9874568581581116), ('주태우', 0.9869372248649597), ('이창환', 0.98565673828125), ('임대운', 0.985440194606781), ('김현우', 0.9849758744239807), ('문봉교', 0.9849180579185486), ('홍정모', 0.984811544418335), ('손윤식', 0.9847736358642578), ('최은만', 0.9847699999809265), ('정영식', 0.9846036434173584), ('정대원', 0.9845662117004395), ('류철', 0.9843294620513916), ('최병석', 0.9842439293861389), ('양기주', 0.9841998815536499), ('김은정', 0.984149694442749), ('윤승현', 0.9841306805610657), ('박미화', 0.9836329817771912), ('임민중', 0.9833610653877258), ('김신우', 0.9833124876022339), ('이용규', 0.9827548265457153), ('신연순', 0.9825058579444885), ('한효준', 0.9824638962745667), ('서정열', 0.9824485778808594), ('정준호', 0.9818634986877441), ('조경은', 0.9818394184112549), ('김동환', 0.9815464019775391), ('송양의', 0.9815235137939453), ('최성연', 0.9814329147338867), ('서상현', 0.9813843965530396), ('주종화', 0.981324315071106), ('박상훈', 0.9803674221038818), ('박경원', 0.9801539182662964), ('장태무', 0.9799534678459167), ('성연식', 0.9797316789627075), ('이강만', 0.9794842600822449), ('정진우', 0.9793891906738281), ('정승원', 0.9786384105682373), ('이강우', 0.9784737229347229), ('김웅섭', 0.9783896207809448), ('김가영', 0.9779452085494995), ('엄진영', 0.9779080152511597), ('조운', 0.9769811630249023), ('오세만', 0.9758907556533813), ('박은찬', 0.9748028516769409), ('이재훈', 0.9746290445327759)]\n"
     ]
    }
   ],
   "source": [
    "print(embed.word2vec.wv.most_similar('김동호', topn=45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedQAData를 통한 단어 vectorize\n",
    "\n",
    "\n",
    "embed.vectorizeWord(word) 함수는 word에 대응하는 벡터를 반환하며, 이는 문자열 분류 딥러닝 모델의 입력으로서 사용 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43749407  0.19403827  0.01953617  0.46005103  0.10975784  0.54906934\n",
      "  -0.30651376  0.02718642  0.08389693  0.22822829  0.4514327  -0.1546101\n",
      "  -0.73814076  0.23028906  0.21702991 -0.21690753 -0.32788274  0.1296224\n",
      "  -0.19887778 -0.41153488  0.38080642  0.19134855 -0.33456147 -0.21013761\n",
      "  -0.10433828  0.51776755  0.23274334 -0.23867849  0.05514818 -0.02785624\n",
      "  -0.56283003 -0.14178321 -0.1170719  -0.03393989  0.32663324 -0.10419425\n",
      "   0.21866202 -0.28924525 -0.38299119 -0.02399184 -0.25657564 -0.04598802\n",
      "   0.39985606  0.29622754 -0.27895808 -0.0392534  -0.43190688 -0.37441874\n",
      "   0.15400191 -0.39620721 -0.79515046 -0.25475162 -0.38195917 -0.02682945\n",
      "  -0.08283545 -0.0504769  -0.1838235   0.15670744  0.1259345   0.354864\n",
      "  -0.89394176 -0.50966024 -0.17803793 -0.1674211   0.04039023 -0.08191461\n",
      "   0.15071036 -0.08209065  0.02674407  0.00483787  0.05701429  0.49547362\n",
      "  -0.19398405 -0.21337189  0.16053754  0.24728402  0.23066407 -0.1833156\n",
      "   0.08786409  0.42928508 -0.01716709 -0.04236657  0.30406263  0.05063574\n",
      "   0.4402799   0.03304514 -0.37836108 -0.17135271 -0.21320374 -0.1728072\n",
      "  -0.12298477 -0.23020269  0.24200724  0.05878548 -0.21229361  0.13178077\n",
      "  -0.20636591  0.26837361 -0.11522663  0.12031163]\n",
      " [-0.48608604  0.18747871  0.03036319  0.4948062   0.08735691  0.59643316\n",
      "  -0.41139433  0.02054119  0.05393834  0.23838024  0.47527319 -0.1826209\n",
      "  -0.77383298  0.26659018  0.20679273 -0.09725316 -0.24556716 -0.06191776\n",
      "  -0.14416127 -0.26901773  0.28667068  0.20953588 -0.31747946 -0.28868145\n",
      "  -0.05138574  0.48948181  0.26132089 -0.21977785  0.05840593 -0.10483339\n",
      "  -0.46701422 -0.31068942 -0.1299516   0.00248163  0.29333892 -0.09457821\n",
      "   0.21134426 -0.29001874 -0.44069895 -0.0187465  -0.22450882 -0.16289032\n",
      "   0.35754231  0.26669767 -0.37195846 -0.06226975 -0.42014337 -0.42462635\n",
      "   0.18768819 -0.3556312  -0.78656673 -0.1655342  -0.36966768 -0.15847595\n",
      "  -0.14975007 -0.14710543 -0.26283893  0.20272493  0.18135972  0.34424099\n",
      "  -0.94857568 -0.57611638 -0.19057167 -0.12156633  0.1154751  -0.1133344\n",
      "   0.10249615 -0.11489598  0.04594838  0.00896128  0.08907082  0.45729133\n",
      "  -0.17359942 -0.20344432  0.26210719  0.13088907  0.32818332 -0.11116049\n",
      "   0.08569148  0.49006897 -0.08226862 -0.0459186   0.31776643 -0.00928936\n",
      "   0.43748426 -0.02451593 -0.49496168 -0.18674442 -0.29928938 -0.14142928\n",
      "  -0.18233249 -0.28162003  0.22318035  0.02583315 -0.23857738  0.09789399\n",
      "  -0.27560917  0.31293923 -0.16232541  0.16882034]]\n"
     ]
    }
   ],
   "source": [
    "temp = ['이강우']\n",
    "\n",
    "print(embed.vectorizeWord(temp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
